{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import delayed,Parallel\n",
    "from nibabel import freesurfer as fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "import re\n",
    "import cPickle as pkl\n",
    "from sklearn.feature_selection import variance_threshold\n",
    "from sklearn.utils import resample\n",
    "from joblib import Parallel,delayed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection.univariate_selection import (\n",
    "    check_X_y,safe_sparse_dot,issparse,row_norms,stats)\n",
    "from pandas.core.algorithms import rank\n",
    "import time\n",
    "idx=pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject directory and subject list\n",
    "gitpath=os.getcwd()\n",
    "SUBJECTS_DIR=(\n",
    "    '/Volumes/Users/mbkranz/projects/'\n",
    "    'ACT_Freesurfer_NewProc/'\n",
    ")\n",
    "os.chdir(SUBJECTS_DIR)\n",
    "sublist=!ls -d ACT*_1\n",
    "os.chdir(gitpath)\n",
    "idx=pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_beta(X,Y):\n",
    "    '''uses the equation (X'X)^(-1)X'Y \n",
    "    to calculate OLS coefficient estimates:\n",
    "    equivalent to the first element \n",
    "    in the built in OLS fxn: np.linalg.lstsq\n",
    "    X : features to predict Y\n",
    "    Y : target variable(s)\n",
    "    \n",
    "    Returns an np.array of num vars\n",
    "    of X by number of vars of Y\n",
    "    '''\n",
    "    n=len(X)\n",
    "    X = np.column_stack([np.ones(n),np.array(X)])\n",
    "    inv_dot_XX=np.linalg.inv(np.dot(X.T,X))\n",
    "    dot_XY=np.dot(X.T,Y)\n",
    "    betas=np.dot(inv_dot_XX,dot_XY)\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr(X,y,spearman=True,center=True):\n",
    "    '''\n",
    "    this is taken from sklearn f_regression except option to rank\n",
    "    variables and outputs correlations instead of f values\n",
    "    '''\n",
    "    if spearman==True:\n",
    "        X=rank(X)\n",
    "        y=rank(y)\n",
    "    #allow y to be multi-dimensional\n",
    "    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], \n",
    "                     dtype=np.float64)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # compute centered values\n",
    "    # note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], \n",
    "    # so we need not center X\n",
    "    if center:\n",
    "        y = y - np.mean(y)\n",
    "        if issparse(X):\n",
    "            X_means = X.mean(axis=0).getA1()\n",
    "        else:\n",
    "            X_means = X.mean(axis=0)\n",
    "        # compute the scaled standard deviations via moments\n",
    "        X_norms = np.sqrt(row_norms(X.T, squared=True) -\n",
    "                          n_samples * X_means ** 2)\n",
    "    else:\n",
    "        X_norms = row_norms(X.T)\n",
    "\n",
    "    # compute the correlation\n",
    "    corr = safe_sparse_dot(y, X)\n",
    "    corr /= X_norms\n",
    "    corr /= np.linalg.norm(y)\n",
    "    #no pv needed as stats are done with bootstrap\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pcorr(X,y,covars):\n",
    "    '''Extracts the residuals of covars on y\n",
    "    and then computes the correlation \n",
    "    (also called a part correlation. \n",
    "    In other words, what is the relationship\n",
    "    of X on Y after accounting for covars?\n",
    "    X : 2-D np.array of observation X features\n",
    "    Y : 1-D array of target variable\n",
    "    covars : 1 or 2 D array of covariates to account for\n",
    "    before running X~Y (residuals) correlation'''\n",
    "    model=LinearRegression()\n",
    "    model.fit(covars,y)\n",
    "    resids=y-model.predict(covars)\n",
    "    pcorr=compute_corr(X,resids)\n",
    "    return pcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mediate(mediatevar_mat,xvar_vec,covar_vec,yvar_vec):\n",
    "    '''\n",
    "    Note: make code to run with 1 or more mediator variables\n",
    "    runs:\n",
    "    1. direct effect or \n",
    "    yvar_vec=intercept+beta_c(xvar_vec) \n",
    "    e.g., Age~intercept+betaCognition\n",
    "    2. full model\n",
    "    run full model on each variable to get beta of xvar_vec\n",
    "    when mediating variable is included\n",
    "    (if more than one variable, will loop through)\n",
    "\n",
    "\n",
    "    mediatevar_mat : a subject x Nvars numpy array of third\n",
    "    variable (mediating variable)\n",
    "    xvar_vec : subject-wise numpy vector of the first variable in \n",
    "    mediation of xvar_vec on yvar_vec\n",
    "    yvar_vec : subject wise numpy vector of second variable of\n",
    "    mediation of xvar_vec on yvar_vec\n",
    "    covar_vec : subject-wise numpy vector of covariates to include\n",
    "    (control for the effect of nuisance variables of no interest on \n",
    "    the relationship of xvar_vec on other variables)\n",
    "    '''\n",
    "    def compute_beta_withmediator():\n",
    "        # try/except (for medial wall with all zero vals)\n",
    "        X_mat=np.column_stack([xvar_vec,covar_vec,mediatevar_vec])\n",
    "        try:\n",
    "            beta_xvar=compute_beta(X_mat,yvar_vec)\n",
    "        except:\n",
    "            beta_xvar=np.repeat(np.nan,X_mat.shape[1])\n",
    "        return beta_xvar[1]\n",
    "\n",
    "    def compute_beta_withoutmediator():\n",
    "        X_mat=np.column_stack([xvar_vec,covar_vec])\n",
    "        beta_xvar=compute_beta(X_mat,yvar_vec)\n",
    "        return beta_xvar[1]\n",
    "    \n",
    "    try:\n",
    "        beta_c=compute_beta_withoutmediator()\n",
    "        mediatevar_iter=np.nditer(mediatevar_mat,\n",
    "                                  flags=['external_loop'],\n",
    "                                  order='F')\n",
    "        beta_cprime=np.array([compute_beta_withmediator()\n",
    "                              for mediatevar_vec in mediatevar_mat.T])\n",
    "        indirect=beta_c-beta_cprime\n",
    "    except:\n",
    "        indirect=np.nan\n",
    "    return indirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a boot class object to update\n",
    "class RollingBootStats:\n",
    "    '''Keeps all boot strap statistics when iterating \n",
    "    across bootstrapped samples.\n",
    "    numsamples : total number of bootstrapped samples\n",
    "    stat_labels: can provide an index to make \n",
    "    a pandas df with after analysis is done (or other labels to \n",
    "    identify the object from other Rolling BootStats objects)'''\n",
    "    def __init__(self,stat_labels,numsamples=1):\n",
    "        self.numsamples=numsamples\n",
    "        self.stat_labels=stat_labels\n",
    "        \n",
    "    def update(self,bootobj,num_boot):\n",
    "        self.num_boot=num_boot\n",
    "        if num_boot==0: #whole sample\n",
    "            self.empirical_mean=bootobj\n",
    "        elif num_boot==1: #first boot sample, initiate objs\n",
    "            self.bootobj_sum=bootobj\n",
    "            self.bootobj_sum_diff2=0\n",
    "        else: #compute rolling stats\n",
    "            self.bootobj_sum=np.add(self.bootobj_sum,bootobj)\n",
    "            bootobj_mean=self.bootobj_sum/num_boot\n",
    "            self.bootobj_sum_diff2+=(\n",
    "                bootobj - bootobj_mean)*(bootobj - bootobj_mean)\n",
    "        if num_boot==self.numsamples:\n",
    "            self.bootobj_mean=bootobj_mean\n",
    "            self.bootobj_var=(self.bootobj_sum_diff2 / \n",
    "                              (num_boot - 1))\n",
    "            self.bootobj_sd=np.sqrt(self.bootobj_var)\n",
    "    def add_bootsample(self,bootobj,num_boot):\n",
    "        if num_boot==1:\n",
    "            self.replications=bootobj\n",
    "        elif num_boot>1:\n",
    "            self.replications=np.vstack([self.replications,bootobj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resampled_indices(npdata,numboot):\n",
    "    '''resample with replacement the npdata\n",
    "    from the csv file and extract the identifiers \n",
    "    across datasets (e.g., subs_resampled). These\n",
    "    will be reused for the brain data indices.\n",
    "    Make sure they are the same!!!!!'''\n",
    "    if numboot==0:\n",
    "        return npdata.index.values.copy(),npdata.copy()\n",
    "    else:\n",
    "        npdata_resampled=resample(npdata.copy())\n",
    "        subs_resampled=npdata_resampled.index.values\n",
    "        return subs_resampled,npdata_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_format_cv_predict_df(cv_predict_path,threshtype=None):\n",
    "    groupby_vars=['subs','measure','network','type','threshold','np_measure_name']\n",
    "    df=pd.read_csv(cv_predict_path)\n",
    "    df['type']=threshtype\n",
    "    df['threshold']=np.round(df['threshold'],6).astype(str)\n",
    "    df['network']=df['pkldir'].str.extract('(network\\d)',expand=False)\n",
    "    df['measure']=df['pkldir'].str.extract('(thickness|area)',expand=False)\n",
    "    df['predictions']=np.where(df['predictions'].isnull(),0,df['predictions'])\n",
    "    del df['pkldir']\n",
    "    df_return=(df.filter(groupby_vars+['predictions'])\n",
    "               .groupby(groupby_vars)\n",
    "               .mean())\n",
    "    return df_return['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_format_cv_predict_df_splits(cv_predict_path,threshtype=None):\n",
    "    groupby_vars=['cviter', 'cvsplit','subs','testindex',\n",
    "                 'measure','network','type','threshold','np_measure_name']\n",
    "    df_withnans=pd.read_csv(cv_predict_path)\n",
    "    nans=df_withnans.predictions.isnull()\n",
    "    df=df_withnans.loc[~nans,:].copy()\n",
    "    df['type']=threshtype\n",
    "    df['threshold']=np.round(df['threshold'],6).astype(str)\n",
    "    df['network']=df['pkldir'].str.extract('(network\\d)',expand=False)\n",
    "    df['measure']=df['pkldir'].str.extract('(thickness|area)',expand=False)\n",
    "    del df['pkldir']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stats_dict(measure,np_name,morph_df,numsamples=1):\n",
    "    '''Initiates a RollingBootStats object after calculating the \n",
    "    number of columns (analyses) in the object.\n",
    "    This is necessary as we need to split the dataset based on\n",
    "    morphometry measures and cognitive measures before running analyses\n",
    "    and each will a different number of columns \n",
    "    (as some thresholds etc were not included if they didn't\n",
    "    #have any featuers selected in cv pipelinepredictions)'''\n",
    "    morph_columns=morph_df.loc[:,idx[measure,:,:,:,np_name]].columns\n",
    "    rollingstats=RollingBootStats(morph_columns,numsamples)\n",
    "    return rollingstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in npdata and morphometry predictions obtained from cv_results\n",
    "#and processed in make_cv_dfs.R script\n",
    "npdatafilt=pd.read_csv('../data/np_filter_wb_gendernum.csv',\n",
    "                       na_values='NA',index_col=\"subs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_vars=['subs','measure','network','type','threshold','np_measure_name']\n",
    "cv_predict_df_fpr=load_format_cv_predict_df(\n",
    "    '../data/cv_results/cv_predict_fpr_df_3fold_4_5_18.csv','fpr')\n",
    "#filter out analyses that had no features selected on at least one fold \n",
    "to_include=((cv_predict_df_fpr==0).groupby(groupby_vars[1:]).sum()==0)\n",
    "cv_predict_df_fpr=(\n",
    "    cv_predict_df_fpr\n",
    "    .reset_index(['subs'])\n",
    "    .loc[to_include.loc[to_include==True].index.values]\n",
    "    .reset_index()\n",
    "    .set_index(groupby_vars)\n",
    "    ['predictions'])\n",
    "\n",
    "cv_predict_df_fpr_avg=(\n",
    "    cv_predict_df_fpr\n",
    "    .groupby(level=[x for x in groupby_vars if x!='threshold'])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .assign(threshold='average')\n",
    "    .set_index(groupby_vars)\n",
    "    ['predictions'])\n",
    "\n",
    "cv_predict_df_fpr_all=pd.concat([cv_predict_df_fpr,cv_predict_df_fpr_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predict_df_fpr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predict_df_percentile=load_format_cv_predict_df(\n",
    "    '../data/cv_results/cv_predict_percentile_df_5fold_4_4_18.csv','percentile')\n",
    "cv_predict_df_all=pd.concat((cv_predict_df_fpr_all,cv_predict_df_percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unstack predictions (so rows are subjects) \n",
    "#unstacked columns need to be fully lexasorted for later slicing\n",
    "cv_predict_df_unstack=(cv_predict_df_fpr_all\n",
    "                       .unstack(groupby_vars[1:])\n",
    "                       .sort_index(axis=1,level=[0,1,2,3,4]))\n",
    "cv_predict_df_unstack.index=pd.Index(\n",
    "    cv_predict_df_unstack.index.values.astype(int),name='subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_statdf(name):\n",
    "    stat_regex='^(area|thickness)_(Memory|ExFunction)_'\n",
    "    if name=='lower':\n",
    "        stat_str='np.percentile(stat.replications,axis=0,q=5)'\n",
    "    elif name=='upper':\n",
    "        stat_str='np.percentile(stat.replications,axis=0,q=95)'\n",
    "    else:\n",
    "        stat_str='stat.'+name\n",
    "    print(stat_str)\n",
    "    stat_df=pd.concat([(pd.DataFrame(eval(stat_str),index=stat.stat_labels)\n",
    "                        .assign(analysis=re.sub(stat_regex,'',key),\n",
    "                                stat=name))\n",
    "                       for key,stat in bootstats_dict.iteritems()])\n",
    "    return stat_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsamples=2000\n",
    "num_boot=0\n",
    "#number of analyses --> numcols in each sliced df \n",
    "#(slicing on np_name and cog)\n",
    "\n",
    "bootstats_dict={meas+'_'+np_name+'_'+anal:init_stats_dict(\n",
    "    meas,np_name,cv_predict_df_unstack,numsamples)\n",
    "                for meas in measure_list \n",
    "                for np_name in np_name_list \n",
    "                for anal in anal_list}\n",
    "for samplenum in range(numsamples+1):\n",
    "    ###resampled npdata and morph data\n",
    "    subs,npdata=get_resampled_indices(npdatafilt,num_boot)\n",
    "    age=-npdata['Age'].values\n",
    "    gender=npdata[['Gender']].values\n",
    "    #loop through each cognitive measure and morph measure\n",
    "    for np_name in np_name_list:\n",
    "        cognitive=npdata[np_name].values\n",
    "        for measure in measure_list:\n",
    "            ###resampled whole brain averages \n",
    "            ###(specific to each measure)\n",
    "            gender_wholebrain=npdata[\n",
    "                ['Gender','_'.join(['wholebrain',measure])]].values\n",
    "            #slice cognitive and measure in cv_predict data columns\n",
    "            cv_predict=(cv_predict_df_unstack\n",
    "                        .loc[subs,idx[measure,:,:,:,np_name]]\n",
    "                        .values)\n",
    "            #get list of analysis names\n",
    "            #to get BootRollingStats objects from dict\n",
    "            anals=[measure+'_'+np_name+'_'+x for x in anal_list]\n",
    "            ##compute bootstrap measures\n",
    "            bootstats=[\n",
    "                ###simple correlations\n",
    "                compute_corr(X=cv_predict,y=age),\n",
    "                compute_corr(X=cv_predict,y=cognitive),\n",
    "                ##gender only covar models\n",
    "                compute_pcorr(covars=gender,X=cv_predict,y=age),\n",
    "                compute_pcorr(covars=gender,X=cv_predict,y=cognitive),\n",
    "                run_mediate(xvar_vec=age,yvar_vec=cognitive,\n",
    "                            mediatevar_mat=cv_predict,\n",
    "                            covar_vec=gender),\n",
    "                ##gender + wb covar models\n",
    "                compute_pcorr(covars=gender_wholebrain,X=cv_predict,y=age),\n",
    "                compute_pcorr(covars=gender_wholebrain,X=cv_predict,y=cognitive),\n",
    "                run_mediate(xvar_vec=age,yvar_vec=cognitive,\n",
    "                            mediatevar_mat=cv_predict,\n",
    "                            covar_vec=gender_wholebrain)\n",
    "            ]\n",
    "            #add boot stats to RollingBootStats objects\n",
    "            for numanal in range(len(anal_list)):\n",
    "                bootstats_dict[anals[numanal]].update(bootstats[numanal],num_boot)\n",
    "                bootstats_dict[anals[numanal]].add_bootsample(bootstats[numanal],\n",
    "                                                              num_boot)\n",
    "    num_boot+=1 #add a boot sample to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine and format data\n",
    "boot_stats_names=['empirical_mean','bootobj_mean','bootobj_sd','lower','upper']\n",
    "boot_stats_list=[make_statdf(x) for x in boot_stats_names]\n",
    "boot_stats_df=(pd.concat(boot_stats_list)\n",
    "               .set_index(['stat','analysis'],append=True)\n",
    "               [0]\n",
    "              .sort_index(level=range(6))\n",
    "              .unstack(['stat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_stats_df.to_csv('../data/stats/cv_predict_models_5Fold.csv',na_rep='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wholebrain analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compute bootstrap measures\n",
    "def run_parallel_bootstats(measure,np_name,anal):\n",
    "    age=-npdata['Age'].values\n",
    "    gender=npdata[['Gender']].values\n",
    "    cognitive=npdata[np_name].values\n",
    "    gender_wholebrain=npdata[\n",
    "    ['Gender','_'.join(['wholebrain',measure])]].values\n",
    "    wholebrain=wholebrain_dict[measure].loc[subs,:].values\n",
    "    if anal=='mediate':\n",
    "        stat_arr=run_mediate(xvar_vec=age,yvar_vec=cognitive,\n",
    "                mediatevar_mat=wholebrain,\n",
    "                covar_vec=gender)\n",
    "    elif anal=='pcorr_age':\n",
    "        stat_arr=compute_pcorr(covars=gender,X=wholebrain,y=age)\n",
    "    elif anal=='pcorr_cog':\n",
    "        stat_arr=compute_pcorr(covars=gender,X=wholebrain,y=cognitive)\n",
    "    elif anal=='corr_cog':\n",
    "        stat_arr=compute_corr(X=wholebrain,y=cognitive)\n",
    "    else:\n",
    "        raise ValueError('Invalid stat name')\n",
    "    return (measure+'_'+np_name+'_'+anal,stat_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in npdata and morphometry predictions obtained from cv_results\n",
    "#and processed in make_cv_dfs.R script\n",
    "npdatafilt=pd.read_csv('../data/np_filter_wb_gendernum.csv',\n",
    "                       na_values='NA',index_col=\"subs\")\n",
    "wholebrain_dict={'thickness':(pd.read_pickle('/Volumes/Users/mbkranz/projects/ACT_Freesurfer_NewProc/'\n",
    "                     'networks_ML/thickness_fwhm10_wholebrainfsaverage_df.pkl')\n",
    "                              .reset_index(['measure'],drop=True)),\n",
    "                 'area':(pd.read_pickle('/Volumes/Users/mbkranz/projects/ACT_Freesurfer_NewProc/'\n",
    "                     'networks_ML/area_fwhm10_wholebrainfsaverage_df.pkl')\n",
    "                              .reset_index(['measure'],drop=True))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_list=['area','thickness']\n",
    "np_name_list=['Memory','ExFunction']\n",
    "hemilist=['rh','lh']\n",
    "bootlist=['bootscore','bootmean']\n",
    "#anal_list=['corr_cog','pcorr_age','pcorr_cog','mediate']\n",
    "anal_list=['pcorr_age','pcorr_cog','mediate']\n",
    "meas_key={'thickness':np.int(0),'area':np.int(1)}\n",
    "hemi_key={'lh':np.int(0),'rh':np.int(1)}\n",
    "numsamples=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 stats took 35.1173670292\n",
      "sample 10 stats took 29.2049238682\n",
      "sample 20 stats took 29.2915360928\n",
      "sample 30 stats took 29.7610719204\n",
      "sample 40 stats took 29.0375299454\n",
      "sample 50 stats took 28.7218580246\n",
      "sample 60 stats took 32.8372189999\n",
      "sample 70 stats took 29.2996580601\n",
      "sample 80 stats took 29.0019950867\n",
      "sample 90 stats took 29.2516129017\n",
      "sample 100 stats took 29.226017952\n",
      "sample 110 stats took 29.2366819382\n",
      "sample 120 stats took 29.4712810516\n",
      "sample 130 stats took 29.8404150009\n",
      "sample 140 stats took 28.5745849609\n",
      "sample 150 stats took 29.7118859291\n",
      "sample 160 stats took 28.8551058769\n",
      "sample 170 stats took 29.0228209496\n",
      "sample 180 stats took 28.853012085\n",
      "sample 190 stats took 28.644646883\n",
      "sample 200 stats took 28.7501018047\n",
      "sample 210 stats took 28.6975400448\n",
      "sample 220 stats took 29.2881319523\n",
      "sample 230 stats took 28.863008976\n",
      "sample 240 stats took 28.7705690861\n",
      "sample 250 stats took 28.7851390839\n",
      "sample 260 stats took 28.2539288998\n",
      "sample 270 stats took 29.0498218536\n",
      "sample 280 stats took 28.8675789833\n",
      "sample 290 stats took 29.0155749321\n",
      "sample 300 stats took 28.6681609154\n",
      "sample 310 stats took 28.8134000301\n",
      "sample 320 stats took 29.2807049751\n",
      "sample 330 stats took 28.959197998\n",
      "sample 340 stats took 29.1232850552\n",
      "sample 350 stats took 28.5625770092\n",
      "sample 360 stats took 29.3917810917\n",
      "sample 370 stats took 29.0874910355\n",
      "sample 380 stats took 27.9931271076\n",
      "sample 390 stats took 28.389770031\n",
      "sample 400 stats took 28.6997251511\n",
      "sample 410 stats took 29.447740078\n",
      "sample 420 stats took 29.0672709942\n",
      "sample 430 stats took 28.4266500473\n",
      "sample 440 stats took 28.3257551193\n",
      "sample 450 stats took 28.9197609425\n",
      "sample 460 stats took 29.078250885\n",
      "sample 470 stats took 28.6259138584\n",
      "sample 480 stats took 28.5801589489\n",
      "sample 490 stats took 28.9327030182\n",
      "sample 500 stats took 29.3810300827\n",
      "sample 510 stats took 28.7754340172\n",
      "sample 520 stats took 28.9613659382\n",
      "sample 530 stats took 28.5773999691\n",
      "sample 540 stats took 28.595181942\n",
      "sample 550 stats took 28.7461898327\n",
      "sample 560 stats took 28.8876550198\n",
      "sample 570 stats took 28.8597319126\n",
      "sample 580 stats took 29.5654709339\n",
      "sample 590 stats took 28.5629520416\n",
      "sample 600 stats took 28.6542069912\n",
      "sample 610 stats took 28.7479159832\n",
      "sample 620 stats took 29.0376439095\n",
      "sample 630 stats took 28.8350560665\n",
      "sample 640 stats took 28.2328948975\n",
      "sample 650 stats took 28.9350390434\n",
      "sample 660 stats took 28.9794039726\n",
      "sample 670 stats took 29.048112154\n",
      "sample 680 stats took 28.6476302147\n",
      "sample 690 stats took 28.8076360226\n",
      "sample 700 stats took 28.7991139889\n",
      "sample 710 stats took 29.8815639019\n",
      "sample 720 stats took 29.330258131\n",
      "sample 730 stats took 29.2640900612\n",
      "sample 740 stats took 28.7817440033\n",
      "sample 750 stats took 29.3668589592\n",
      "sample 760 stats took 29.1759569645\n",
      "sample 770 stats took 29.8375599384\n",
      "sample 780 stats took 28.9378709793\n",
      "sample 790 stats took 28.9751091003\n",
      "sample 800 stats took 28.7536590099\n",
      "sample 810 stats took 28.7787718773\n",
      "sample 820 stats took 28.800276041\n",
      "sample 830 stats took 28.8513081074\n",
      "sample 840 stats took 28.8249959946\n",
      "sample 850 stats took 28.7751021385\n",
      "sample 860 stats took 28.8817288876\n",
      "sample 870 stats took 29.4369912148\n",
      "sample 880 stats took 29.9107120037\n",
      "sample 890 stats took 28.952507019\n",
      "sample 900 stats took 28.6901898384\n",
      "sample 910 stats took 28.6753919125\n",
      "sample 920 stats took 29.3769140244\n",
      "sample 930 stats took 28.9479680061\n",
      "sample 940 stats took 29.4485080242\n",
      "sample 950 stats took 28.9365549088\n",
      "sample 960 stats took 29.1742370129\n",
      "sample 970 stats took 29.1400520802\n",
      "sample 980 stats took 28.8627901077\n",
      "sample 990 stats took 29.2753880024\n",
      "sample 1000 stats took 28.9373009205\n",
      "sample 1010 stats took 28.6508150101\n",
      "sample 1020 stats took 28.901088953\n",
      "sample 1030 stats took 28.6296961308\n",
      "sample 1040 stats took 29.1180419922\n",
      "sample 1050 stats took 28.912899971\n",
      "sample 1060 stats took 28.6941571236\n",
      "sample 1070 stats took 28.7397100925\n",
      "sample 1080 stats took 28.5380210876\n",
      "sample 1090 stats took 28.5025949478\n",
      "sample 1100 stats took 28.7642459869\n",
      "sample 1110 stats took 29.3432810307\n",
      "sample 1120 stats took 29.2003180981\n",
      "sample 1130 stats took 28.7334020138\n",
      "sample 1140 stats took 28.9875068665\n",
      "sample 1150 stats took 29.6379959583\n",
      "sample 1160 stats took 28.8550767899\n",
      "sample 1170 stats took 29.0098869801\n",
      "sample 1180 stats took 28.5816729069\n",
      "sample 1190 stats took 29.1417160034\n",
      "sample 1200 stats took 28.8080451488\n",
      "sample 1210 stats took 28.9410319328\n",
      "sample 1220 stats took 28.9567630291\n",
      "sample 1230 stats took 29.1423661709\n",
      "sample 1240 stats took 29.1235220432\n",
      "sample 1250 stats took 28.9192860126\n",
      "sample 1260 stats took 28.6552250385\n",
      "sample 1270 stats took 29.2782239914\n",
      "sample 1280 stats took 28.7023420334\n",
      "sample 1290 stats took 28.6310460567\n",
      "sample 1300 stats took 28.9347140789\n",
      "sample 1310 stats took 28.7647647858\n",
      "sample 1320 stats took 29.0562520027\n",
      "sample 1330 stats took 28.5185201168\n",
      "sample 1340 stats took 27.9720599651\n",
      "sample 1350 stats took 29.100481987\n",
      "sample 1360 stats took 29.0930738449\n",
      "sample 1370 stats took 28.8790810108\n",
      "sample 1380 stats took 28.783052206\n",
      "sample 1390 stats took 29.5557699203\n",
      "sample 1400 stats took 28.7654490471\n",
      "sample 1410 stats took 28.937169075\n",
      "sample 1420 stats took 28.6593849659\n",
      "sample 1430 stats took 28.8633999825\n",
      "sample 1440 stats took 28.306636095\n",
      "sample 1450 stats took 29.2157139778\n",
      "sample 1460 stats took 28.4413740635\n",
      "sample 1470 stats took 28.8000187874\n",
      "sample 1480 stats took 28.7898590565\n",
      "sample 1490 stats took 29.6422810555\n",
      "sample 1500 stats took 28.8942651749\n",
      "sample 1510 stats took 29.0452771187\n",
      "sample 1520 stats took 29.2163648605\n",
      "sample 1530 stats took 28.8814849854\n",
      "sample 1540 stats took 29.205616951\n",
      "sample 1550 stats took 28.8713839054\n",
      "sample 1560 stats took 28.6671590805\n",
      "sample 1570 stats took 29.575138092\n",
      "sample 1580 stats took 28.7573590279\n",
      "sample 1590 stats took 28.6568710804\n",
      "sample 1600 stats took 29.0216400623\n",
      "sample 1610 stats took 29.6035990715\n",
      "sample 1620 stats took 28.8674669266\n",
      "sample 1630 stats took 29.6522870064\n",
      "sample 1640 stats took 28.9105620384\n",
      "sample 1650 stats took 28.6234750748\n",
      "sample 1660 stats took 29.3632359505\n",
      "sample 1670 stats took 28.9788651466\n",
      "sample 1680 stats took 28.6379477978\n",
      "sample 1690 stats took 29.6151990891\n",
      "sample 1700 stats took 28.8413419724\n",
      "sample 1710 stats took 28.9218170643\n",
      "sample 1720 stats took 28.9811890125\n",
      "sample 1730 stats took 28.9550788403\n",
      "sample 1740 stats took 28.7049620152\n",
      "sample 1750 stats took 28.9076740742\n",
      "sample 1760 stats took 29.3551669121\n",
      "sample 1770 stats took 29.1046419144\n",
      "sample 1780 stats took 28.840873003\n",
      "sample 1790 stats took 29.0962741375\n",
      "sample 1800 stats took 29.0284910202\n",
      "sample 1810 stats took 29.2950999737\n",
      "sample 1820 stats took 29.0049829483\n",
      "sample 1830 stats took 28.6570730209\n",
      "sample 1840 stats took 28.6573939323\n",
      "sample 1850 stats took 29.1969339848\n",
      "sample 1860 stats took 28.5942239761\n",
      "sample 1870 stats took 29.4212708473\n",
      "sample 1880 stats took 28.9068381786\n",
      "sample 1890 stats took 29.1291100979\n",
      "sample 1900 stats took 28.9524521828\n",
      "sample 1910 stats took 28.5507159233\n",
      "sample 1920 stats took 29.1555819511\n",
      "sample 1930 stats took 29.2657649517\n",
      "sample 1940 stats took 29.0326900482\n",
      "sample 1950 stats took 28.9136500359\n",
      "sample 1960 stats took 28.8029589653\n",
      "sample 1970 stats took 29.0322091579\n",
      "sample 1980 stats took 30.0730130672\n",
      "sample 1990 stats took 28.9702000618\n",
      "sample 2000 stats took 29.2578101158\n"
     ]
    }
   ],
   "source": [
    "bootstats_dict={meas+'_'+np_name+'_'+anal:RollingBootStats(\n",
    "        wholebrain_dict[meas].columns,numsamples)\n",
    "                for meas in meas_key.iterkeys()\n",
    "                for np_name in np_name_list \n",
    "                for anal in anal_list}\n",
    "for num_boot in range(numsamples+1):\n",
    "    ###resampled npdata and morph data\n",
    "    subs,npdata=get_resampled_indices(npdatafilt,num_boot)\n",
    "    #loop through each cognitive, morph, and stat measure\n",
    "    #in parallel as it takes around 32 sec to run through \n",
    "    #the 3 stats6\n",
    "    starttime=time.time()\n",
    "    bootstats_list=Parallel(12)(delayed(run_parallel_bootstats)\n",
    "                (measure,np_name,anal)\n",
    "                for measure in meas_key.iterkeys()\n",
    "                for np_name in np_name_list \n",
    "                for anal in anal_list)\n",
    "    endtime=time.time()\n",
    "    if num_boot%10==0:\n",
    "        print('sample '+str(num_boot) + \n",
    "              ' stats took '+str(endtime-starttime))\n",
    "    #add boot stats to RollingBootStats objects\n",
    "    for anal_name,anal in bootstats_list:\n",
    "        (bootstats_dict[anal_name]\n",
    "         .update(anal,num_boot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../data/wholebrain_bootstrap/bootstats_dict_4_17_18.pkl','wb')\n",
    "pkl.dump(bootstats_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getannot(annotname,hemi):\n",
    "    annot_data=fs.read_annot(\n",
    "        '/Applications/freesurfer/'\n",
    "        'subjects/fsaverage/label/' + hemi + '.' + \n",
    "        annotname + '.annot')\n",
    "    annot_hemi=pd.DataFrame({\n",
    "            \"annot_label\" : annot_data[0],\n",
    "            \"vertex_index\" : range(len(annot_data[0]))})\n",
    "    return annot_hemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_curvoverlay(analname,bsobj,hemi,bootmeas,save_as_pickle=True):\n",
    "    hemi_index=hemi_key[hemi]\n",
    "    annots=getannot('Yeo2011_7Networks_N1000',hemi)\n",
    "    \n",
    "    bs_df=pd.DataFrame({'empirical_mean':bsobj.empirical_mean,\n",
    "                  'boot_mean':bsobj.bootobj_mean,\n",
    "                  'boot_sd':bsobj.bootobj_sd},\n",
    "                  index=bsobj.stat_labels)\n",
    "    bs_df['boot_score']=bs_df['empirical_mean']/bs_df['boot_sd']\n",
    "    bs_df_annot=(bs_df.loc[idx[:,hemi_key[hemi],:],:]\n",
    "                .reset_index([0,'hemi'],drop=True)\n",
    "                .join(annots,how='right'))\n",
    "    curv_vals=(bs_df_annot.sort_index()\n",
    "               [bootmeas]\n",
    "               .values)\n",
    "    if save_as_pickle:\n",
    "        bs_df_annot.to_pickle('../data/wholebrain_bootstrap/'+hemi+'_'+analname+'_'+bootmeas+'.pkl')\n",
    "    fs.write_morph_data(file_like=('../curvoverlays/'+\n",
    "                                   hemi+'_'+analname+'_'+bootmeas+'.curv'),\n",
    "                        values=curv_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a146dd8d9b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbootstats_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/wholebrain_bootstrap/bootstats_dict_4_17_18.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/envs/cpac/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \"\"\" This is called upon unpickling, rather than the default which doesn't\n\u001b[1;32m     96\u001b[0m     \u001b[0mhave\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbreaks\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bootstats_dict=pkl.load(open('../data/wholebrain_bootstrap/bootstats_dict_4_17_18.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in ['boot_mean','boot_score']:\n",
    "    for anal_name,stats in bootstats_dict.iteritems():\n",
    "        for hemi in hemi_key.keys():\n",
    "            save_curvoverlay(anal_name,stats,hemi,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
