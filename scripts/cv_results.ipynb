{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__\n",
    "- make cv_run fxn into a class with functions (rather than function with nested functions)\n",
    "- make separate scripts for running cv pipeline and saving as df\n",
    "- continue to add description to README and/or this notebook on workflow of script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outline of script\n",
    "\n",
    "__see methods section of manuscript_inprep.Rmd in root dir of repository for the cross validation pipeline__\n",
    "\n",
    "`cv_run`: loads a specified morphometry pickle file and runs a function and loops through cross validation splits and/or iterations of a function called `cv_evaluation` that extracts cv results and prediction values using a linear regression estimator after univariate feature selection (with a number of options). Outputs these results as a set of lists in pkl file.\n",
    "\n",
    "\n",
    "- can chose the type of feature selection (percentile or fpr)\n",
    "- having a `covarListList` includes covariates in the final model\n",
    "    (e.g., if [['Gender']] is included as a covariate, then the feature selection pipeline will be run for cortical morphometry and then the covariate added after average morphometry is calculated.\n",
    "       - covariates were previously going to be used to control for nuisance variables but this was created to include nested models in order to control for gender, but would be risky to use in publication as I'm not 100% sure how to run inferential tests on\n",
    "- if `run_features`=True extracts the pvalues from the feature selection process across folds.\n",
    "- if npdata_features=True, only uses data from the csv file containing behavioral features (so doesn't run the feature selection process)\n",
    "- `threshList`: can include a list of different thresholds to run (use in combination with the feature selection method to get reasonable threshold values-- e.g., .001 means something different for fpr and percentile)\n",
    "\n",
    "\n",
    "\n",
    "`make_cvdfs`: function that loads the pickle files and from cv_run and makes pandas dataframe and then saves as pandas dataframe file (and used in R for analyses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#general packages used for data import etc\n",
    "from nibabel import freesurfer as fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "import re\n",
    "import cPickle as pkl #using csv now for file formats (except cv indices currently)\n",
    "from joblib import Parallel,delayed\n",
    "#machine learning tools used\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_selection import (f_regression, \n",
    "                                       SelectFpr, \n",
    "                                       SelectPercentile)\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection.univariate_selection import (check_X_y,\n",
    "                                                            safe_sparse_dot,\n",
    "                                                            issparse,\n",
    "                                                            row_norms,stats)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from pandas.core.algorithms import rank\n",
    "import itertools\n",
    "import time\n",
    "#slice indexing\n",
    "idx=pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matplotlib to see distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################directories###################\n",
    "SUBJECTS_DIR=(\n",
    "    '/Volumes/Users/mbkranz/'\n",
    "    'projects/ACT_Freesurfer_NewProc/'\n",
    "datapath='../data/'\n",
    "pkldirList=glob.glob(\n",
    "    SUBJECTS_DIR+\n",
    "    'networks_ML/*network*.pkl'\n",
    ")\n",
    "\n",
    "##model fitting parameters/variables##\n",
    "feature_select_type='fpr'\n",
    "pval_fprList=[.05,.01,.001]\n",
    "perc_List=np.hstack(\n",
    "    [np.linspace(.0001,.9,50),\n",
    "     np.linspace(1,100,50)]\n",
    ")\n",
    "npList=['Memory','ExFunction']\n",
    "allcovarcombosList=[\n",
    "    ['Gender'],\n",
    "    ['Gender','wholebrain']\n",
    "]\n",
    "##data frame names##\n",
    "index_names=[\n",
    "    'cviter',\n",
    "    'cvsplit',\n",
    "    'np_measure_name',\n",
    "    'pkldir',\n",
    "    'covariates',\n",
    "    'threshold'\n",
    "]\n",
    "predict_names=[\n",
    "    'test','subs','prediction'\n",
    "]\n",
    "results_names=[\n",
    "    'test_rsq',\n",
    "    'train_rsq',\n",
    "    'test_mse',\n",
    "    'train_mse',\n",
    "    'num_features_selected',\n",
    "    'time',\n",
    "    'coef',\n",
    "    'intercept'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#don't want to scale dummy variables, \n",
    "#so made a separate transform function to separate\n",
    "#dummy vs. non dummy variables before standardizing\n",
    "#non dummy\n",
    "def _split_data(X,indices):\n",
    "    dummyvals=(\n",
    "        X[:,[i for i, x \n",
    "             in enumerate(indices) \n",
    "             if x]]\n",
    "    )\n",
    "    notdummyvals=(\n",
    "        X[:,[i for i, x \n",
    "             in enumerate(indices) \n",
    "             if not x]]\n",
    "    )\n",
    "    return notdummyvals,dummyvals\n",
    "class StandardScaler_notdummy(BaseEstimator, \n",
    "                              TransformerMixin):\n",
    "    \"\"\"sklearn preprocessing\n",
    "    - standardizes all variables except dummy variables \n",
    "    - i.e., (vars that are 0 and 1)\n",
    "    - e.g., Gender \"\"\"\n",
    "    def _reset(self):\n",
    "        if hasattr(self, 'dummy_indices_'):\n",
    "            del self.dummy_indices_\n",
    "            del self.notdummy_mean_\n",
    "            del self.notdummy_sd_\n",
    "    def fit(self, X, y=None):\n",
    "        self._reset()\n",
    "        self.dummy_indices_=[np.array_equal(np.unique(x),[0,1]) \n",
    "                   for x in X.T]\n",
    "        notdumm,dumm=_split_data(X,self.dummy_indices_)\n",
    "        self.notdummy_mean_=np.mean(notdumm,axis=0)\n",
    "        self.notdummy_sd_=np.std(notdumm,axis=0)\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        notdumm,dumm=_split_data(X,self.dummy_indices_)\n",
    "        if notdumm.shape[1]>0:\n",
    "            notdumm -= self.notdummy_mean_\n",
    "            notdumm /= self.notdummy_sd_\n",
    "            return np.hstack((notdumm,dumm))\n",
    "        else: #only dummy variables\n",
    "            return dumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_corr(X,y,spearman=True,center=True):\n",
    "    '''\n",
    "    this is taken from sklearn f_regression except option to rank\n",
    "    variables and outputs correlations instead of f values\n",
    "    '''\n",
    "    if spearman==True:\n",
    "        X=rank(X)\n",
    "        y=rank(y)\n",
    "\n",
    "    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # compute centered values\n",
    "    # note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we\n",
    "    # need not center X\n",
    "    if center:\n",
    "        y = y - np.mean(y)\n",
    "        if issparse(X):\n",
    "            X_means = X.mean(axis=0).getA1()\n",
    "        else:\n",
    "            X_means = X.mean(axis=0)\n",
    "        # compute the scaled standard deviations via moments\n",
    "        X_norms = np.sqrt(row_norms(X.T, squared=True) -\n",
    "                          n_samples * X_means ** 2)\n",
    "    else:\n",
    "        X_norms = row_norms(X.T)\n",
    "\n",
    "    # compute the correlation\n",
    "    corr = safe_sparse_dot(y, X)\n",
    "    corr /= X_norms\n",
    "    corr /= np.linalg.norm(y)\n",
    "\n",
    "    # convert to p-value\n",
    "    degrees_of_freedom = y.size - (2 if center else 1)\n",
    "    F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n",
    "    pv = stats.f.sf(F, 1, degrees_of_freedom)\n",
    "    return corr, pv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_run(pkldir,cvtype,\n",
    "           feature_select_type=None,\n",
    "           threshList=[1],\n",
    "           covarListList=[''],\n",
    "           npdata_features=False,\n",
    "           npdata_features_names=None,\n",
    "           cviter=1,nfolds=None,\n",
    "           permtest=False, \n",
    "           extra_savefile_str='',\n",
    "           run_features=False,fit_inter=True):\n",
    "\n",
    "    ########cv split method#######\n",
    "    if cvtype=='loo':\n",
    "        cv=[{cog:LeaveOneOut() for cog in npList}]\n",
    "    else:\n",
    "        \n",
    "        cv=list(cogcvs)\n",
    "    ##########load MRI data########\n",
    "    if npdata_features==False:\n",
    "        print(' loading '+pkldir)\n",
    "        i_network_df=pd.read_pickle(pkldir)\n",
    "        i_networkvals=i_network_df.values\n",
    "        print('  starting '+pkldir)\n",
    "    else:\n",
    "        print('running cv for {}'.format(','.join(npdata_features_names)))\n",
    "        i_networkvals=npdata[npdata_features_names].values\n",
    "    ##########run and return cv_features if selected########\n",
    "    def cv_features(np_measure_name,train,test):\n",
    "        y_target=npdata[np_measure_name].values\n",
    "        X_train=i_networkvals[train]\n",
    "        y_train=y_target[train]\n",
    "        X_test=i_networkvals[test]\n",
    "        y_test=y_target[test]\n",
    "        corrs,pvals=f_corr(X_train,y_train)\n",
    "        return tuple([(np_measure_name,pkldir),(corrs,pvals)])\n",
    "    if run_features==True:\n",
    "        cv_features=[\n",
    "            (i,splitnum,\n",
    "             cv_features(cog,splits[0],splits[1]))\n",
    "            for cog in npList \n",
    "            for i in xrange(cviter) \n",
    "            for splitnum,splits \n",
    "            in enumerate(cv[i][cog].split(npdata))\n",
    "        ]\n",
    "        return cv_features\n",
    " \n",
    "    #######define and run cv pipeline##########\n",
    "    def cv_evaluation(np_measure_name,train,test,\n",
    "                      thresh,covarList,scaley=True):\n",
    "        ####make some of the feature selection,transform, estimate objects####\n",
    "        starttime=time.time()\n",
    "        \n",
    "        y_target=npdata[np_measure_name].values\n",
    "        if permtest==True:\n",
    "            np.random.shuffle(y_target)\n",
    "        ######create train and test datasets######\n",
    "        X_train=i_networkvals[train]\n",
    "        y_train=y_target[train]\n",
    "        X_test=i_networkvals[test]\n",
    "        y_test=y_target[test]\n",
    "        if scaley==True:\n",
    "            y_mean=np.mean(y_train)\n",
    "            y_sd=np.std(y_train)\n",
    "            y_train -= y_mean\n",
    "            y_train /= y_sd\n",
    "            y_test -= y_mean\n",
    "            y_test /= y_sd\n",
    "        #concatenate covariate data if covariates\n",
    "        if covarList!='': \n",
    "            measure=re.search(\n",
    "                'area|thickness',\n",
    "                pkldir\n",
    "            ).group(0)\n",
    "            covarList=[x+'_'+measure \n",
    "                       if x=='wholebrain' \n",
    "                       else x \n",
    "                       for x in covarList]\n",
    "            X_train=np.hstack(\n",
    "                (npdata[covarList].values[train],\n",
    "                 X_train)\n",
    "            )\n",
    "            X_test=np.hstack(\n",
    "                (npdata[covarList].values[test],\n",
    "                 X_test)\n",
    "            )\n",
    "        ######construct pipeline##############\n",
    "        #define linear regression estimator with scaling\n",
    "        \n",
    "        linear_est=[\n",
    "            ('scale2',StandardScaler_notdummy()),\n",
    "            ('linear',LinearRegression(\n",
    "                fit_intercept=fit_inter\n",
    "            ))\n",
    "        ]\n",
    "        #if running npdata as features, then only need estimator in pipeline\n",
    "        if npdata_features==True:\n",
    "            cv_pipeline=Pipeline(linear_est)\n",
    "            num_features_selected=(-1)\n",
    "        else:\n",
    "            #define transform fxns\n",
    "            def meansum_transform(x,key):\n",
    "                if any([s in key for s in ['thickness']]):\n",
    "                    trans=np.mean(x,axis=1).reshape(-1,1)\n",
    "                elif 'area' in key:\n",
    "                    trans=np.sum(x,axis=1).reshape(-1,1)\n",
    "                return trans\n",
    "            def selectfeat(x,key,ncov):\n",
    "                if any([s in key for s in ['area','thickness']]):\n",
    "                    trans=x[:,ncov:]\n",
    "                elif key=='covars':\n",
    "                    trans=x[:,:ncov-1]\n",
    "                return trans\n",
    "            \n",
    "            numcovar=len(covarList) #used in selectfeat\n",
    "            selectvertices=FunctionTransformer(\n",
    "                selectfeat,kw_args={'key':pkldir,'ncov':numcovar})\n",
    "            selectcovars=FunctionTransformer(\n",
    "                selectfeat,kw_args={'key':'covars','ncov':numcovar})\n",
    "            if feature_select_type=='fpr':\n",
    "                anova_select=SelectFpr(f_corr,alpha=thresh)\n",
    "            elif feature_select_type=='percentile':\n",
    "                anova_select=SelectPercentile(f_corr,percentile=thresh)\n",
    "                \n",
    "            combine_morph=FunctionTransformer(meansum_transform,\n",
    "                                              kw_args={'key':pkldir})\n",
    "            vertex_pipe=[('selector_vert',selectvertices),\n",
    "                        # ('scale1',RobustScaler()),\n",
    "                         ('anovaselect',anova_select),\n",
    "                         ('combine', combine_morph)]\n",
    "            if numcovar>0:\n",
    "                covars_pipe=[('selector_cov',selectcovars)]\n",
    "                vertex_covars_tuple=[('morph',Pipeline(vertex_pipe)),\n",
    "                                     ('covars',Pipeline(covars_pipe))]\n",
    "                vertex_covars_union=[('union',FeatureUnion(\n",
    "                    transformer_list=vertex_covars_tuple))]\n",
    "            \n",
    "            #if covariates,\n",
    "            #then include the FeatureUnion object\n",
    "            if numcovar>0:\n",
    "                cv_pipeline=Pipeline(vertex_covars_union+linear_est)\n",
    "             #if not covar list \n",
    "            #then just do:\n",
    "                ##feature selection, \n",
    "                ##avg/sum vertices, \n",
    "                ##run estimator\n",
    "            else:\n",
    "                cv_pipeline=Pipeline(vertex_pipe+linear_est) \n",
    "            only_vertices_train=selectvertices.fit_transform(\n",
    "                X_train\n",
    "            )\n",
    "            num_features_selected=anova_select.fit_transform(\n",
    "                X=only_vertices_train,\n",
    "                y=y_train\n",
    "            ).shape[1]\n",
    "        ####run pipeline if greater than 0 features or npdata as features####\n",
    "        ##if there are features select -->\n",
    "        ##there are no features selected (can't fit)\n",
    "        #no features selected for npdata as features so num_feat=NA\n",
    "        if num_features_selected>0 or npdata_features==True:\n",
    "            #fit\n",
    "            cv_pipeline.fit(X=X_train,y=y_train)\n",
    "            #predictions\n",
    "            X_train_pred=cv_pipeline.predict(X_train)\n",
    "            X_test_pred=cv_pipeline.predict(X_test)\n",
    "            #metrics\n",
    "            train_rsq=r2_score(y_train,X_train_pred)\n",
    "            train_mse=mean_squared_error(y_train,X_train_pred)\n",
    "            test_rsq=r2_score(y_test,X_test_pred)\n",
    "            test_mse=mean_squared_error(y_test,X_test_pred)\n",
    "            cv_results=[\n",
    "                test_rsq,\n",
    "                train_rsq, \n",
    "                test_mse, \n",
    "                train_mse,\n",
    "                num_features_selected\n",
    "            ]\n",
    "            cv_predicts=X_test_pred.reshape(-1,1)\n",
    "            \n",
    "            coefs=list(\n",
    "                cv_pipeline\n",
    "                .named_steps\n",
    "                ['linear']\n",
    "                .coef_\n",
    "            )\n",
    "            intercept=[\n",
    "                cv_pipeline\n",
    "                .named_steps\n",
    "                ['linear']\n",
    "                .intercept_\n",
    "            ]\n",
    "        else:\n",
    "            cv_results=[np.nan]*4+[0]\n",
    "            cv_predicts=np.array([[np.nan]]*len(test))\n",
    "            coefs=[np.nan]\n",
    "            intercept=[np.nan]\n",
    "        ##return a list containing \n",
    "        ###1. list of indices \n",
    "        ###2. train/test performance metrics \n",
    "        ###3. prediction values with ids (e.g., subs)\n",
    "        ids_test=ids[test].reshape(-1,1)\n",
    "        index_results=[np_measure_name,\n",
    "                       pkldir,'_'.join(covarList),\n",
    "                       str(thresh)]\n",
    "        return [\n",
    "            index_results,\n",
    "            cv_results+\n",
    "            [time.time()-starttime]+\n",
    "            [coefs]+\n",
    "            intercept,\n",
    "            np.hstack((test.reshape(-1,1),\n",
    "                       ids_test,cv_predicts))\n",
    "        ]\n",
    "    \n",
    "    ##runs cv train test (cv_evaluation) ##\n",
    "    '''\n",
    "    - for each target (i.e., cognitive var)\n",
    "    - feature selection threshold\n",
    "    - covariate\n",
    "    - cv split\n",
    "    - train/test cv performance results for:\n",
    "       - target variables, \n",
    "       - features selection thresholds\n",
    "       - covariates\n",
    "       - for eaceh cv split and iteration\n",
    "    - returns a list of tuples with: \n",
    "        - iteration, split, \n",
    "        - cv_results/predictions/index vars\n",
    "    '''\n",
    "    cv_results=[\n",
    "        (i,splitnum,\n",
    "         [cv_evaluation(\n",
    "             cog,\n",
    "             splits[0],splits[1],\n",
    "             thresh,covarList\n",
    "         ) \n",
    "          for thresh in threshList\n",
    "          for covarList in covarListList])\n",
    "        for cog in npList \n",
    "        for i in xrange(cviter) \n",
    "        for splitnum,splits \n",
    "        in enumerate(cv[i][cog].split(npdata))]\n",
    "    pkldir_save=(\n",
    "        datapath+\n",
    "        'cv_results/tmp/'\n",
    "        '_tmp_cv_results_'+\n",
    "        extra_savefile_str+\n",
    "        '_'.join(\n",
    "            [y \n",
    "             for x in covarListList \n",
    "             for y in x]\n",
    "        )\n",
    "    )\n",
    "    if npdata_features==True:\n",
    "        cv_results_pkldir=(pkldir_save+\n",
    "                           pkldir+\n",
    "                           '.pkl')\n",
    "    else:\n",
    "        cv_results_pkldir=pkldir.replace(\n",
    "            SUBJECTS_DIR+\n",
    "            'networks_ML/',pkldir_save\n",
    "        )\n",
    "    print('saving '+ cv_results_pkldir)\n",
    "    pkl.dump(cv_results,open(cv_results_pkldir,'wb'))\n",
    "    ###################################\n",
    "    return pkldir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save dataframes of cv results\n",
    "def save_cv_dfs(cv_list,\n",
    "                featselectstr,\n",
    "                endstr='_df.csv',\n",
    "                return_data=False,\n",
    "                save_data=True):\n",
    "\n",
    "    cv_list_flat=[\n",
    "        (i,s,y) \n",
    "        for r in cv_list \n",
    "        for i,s,x in r \n",
    "        for y in x\n",
    "    ]\n",
    "    predict_index=list(\n",
    "        itertools\n",
    "        .chain\n",
    "        .from_iterable(\n",
    "            itertools.repeat(\n",
    "                [i,s]+x[0], len(x[2])) \n",
    "            for i,s,x in cv_list_flat\n",
    "        )\n",
    "    )\n",
    "    results_index=[\n",
    "        [i,s]+x[0] \n",
    "        for i,s,x in cv_list_flat\n",
    "    ]\n",
    "    def make_indices(index):\n",
    "        df=pd.DataFrame(index,columns=index_names)\n",
    "        return df.set_index(index_names).index\n",
    "    predict_df=pd.DataFrame(\n",
    "        np.vstack([x[2] for i,s,x in cv_list_flat]),\n",
    "        index=make_indices(predict_index),\n",
    "        columns=predict_names\n",
    "    )\n",
    "    results_df=pd.DataFrame(\n",
    "        [x[1] for i,s,x in cv_list_flat],\n",
    "        index=make_indices(results_index),\n",
    "        columns=results_names\n",
    "    )\n",
    "    if save_data==True:\n",
    "        predict_df.to_csv(\n",
    "            datapath+\n",
    "            'cv_results/'+\n",
    "            'cv_predict_'+\n",
    "            featselectstr+\n",
    "            endstr,\n",
    "            na_rep='NA'\n",
    "        )\n",
    "        results_df.to_csv(\n",
    "            datapath+\n",
    "            'cv_results/'+\n",
    "            'cv_results_'+\n",
    "            featselectstr+\n",
    "            endstr,\n",
    "            na_rep='NA'\n",
    "        )\n",
    "    if return_data==True:\n",
    "        return (results_df,predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import behavioral data (target)\n",
    "#np_all filtered to include good MRI data in MakeDataFrames script\n",
    "npdata=pd.read_csv(\n",
    "    datapath+\n",
    "    'np_filter_wb_gendernum.csv',\n",
    "    na_values='NA',\n",
    "    index_col=\"subs\"\n",
    ")\n",
    "ids=npdata.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create or read in sklearn validation object list\n",
    "#if create makes a unique file with time.time\n",
    "nfolds=5\n",
    "read_folds=True\n",
    "create_folds=False\n",
    "write_folds=False\n",
    "if create_folds==True:\n",
    "    cogcvs=[\n",
    "        {cog:KFold(n_splits=nfolds,\n",
    "                   shuffle=True) \n",
    "         for cog in npList} \n",
    "        for i in xrange(100)\n",
    "    ]\n",
    "if write_folds==True:\n",
    "    pkl.dump(\n",
    "        cogcvs,open(\n",
    "        '../data/cv_results/'\n",
    "        'cvfold_indices_{}fold_{}.pkl'\n",
    "        .format(str(nfolds),str(time.time()))\n",
    "        ),\n",
    "        'wb')\n",
    "if read_folds==True:\n",
    "    cogcvs=pkl.load(open(\n",
    "        '../data/cv_results/'\n",
    "        'cvfold_indices_{}fold.pkl'\n",
    "        .format(str(nfolds)),\n",
    "        'rb'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation with covariates (gender and wholebrain)\n",
    "Parallel(5)(\n",
    "    delayed(cv_run)(\n",
    "        pkldir=pkldir,\n",
    "        feature_select_type='fpr',\n",
    "        threshList=pval_fprList,\n",
    "        covarListList=allcovarcombosList,\n",
    "        cvtype='kfold',\n",
    "        cviter=100\n",
    "    )\n",
    "    for pkldir in pkldirList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation with NPDATA covariates (gender and wholebrain)\n",
    "Parallel(5)(\n",
    "    delayed(cv_run)(\n",
    "        pkldir='npdata_'+'_'.join(names),\n",
    "        npdata_features=True,\n",
    "        npdata_features_names=names,\n",
    "        cvtype='kfold',\n",
    "        cviter=100\n",
    "    )\n",
    "    for names in [\n",
    "        ['Gender'],\n",
    "        ['Gender','wholebrain_thickness'],\n",
    "        ['Gender','wholebrain_area']\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation\n",
    "Parallel(5)(delayed(cv_run)\n",
    "               (pkldir=pkldir,\n",
    "                feature_select_type='fpr',\n",
    "                threshList=pval_fprList,\n",
    "                covarListList=[''],\n",
    "                cvtype='kfold',\n",
    "                cviter=100)\n",
    "               for pkldir in pkldirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation more conservative p values\n",
    "Parallel(5)(delayed(cv_run)\n",
    "               (pkldir=pkldir,\n",
    "                extra_savefile_str='extremepvals',\n",
    "                feature_select_type='fpr',\n",
    "                threshList=[.0005,.0001,.00001],\n",
    "                covarListList=[''],\n",
    "                cvtype='kfold',\n",
    "                cviter=100)\n",
    "               for pkldir in pkldirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running network 2 thickness as Kramerlab is running 3-7 (and done with area)\n",
    "pkldirList_thicknet2=[x for x in pkldirList if re.search('thickness_.*network2',x)]\n",
    "#run cross validation with percentile selection\n",
    "Parallel(5)(delayed(cv_run)\n",
    "               (pkldir=pkldir,\n",
    "                extra_savefile_str='percentile',\n",
    "                feature_select_type='percentile',\n",
    "                threshList=perc_List,\n",
    "                covarListList=[''],\n",
    "                cvtype='kfold',\n",
    "                cviter=100)\n",
    "               for pkldir in pkldirList_thicknet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation with percentile selection\n",
    "Parallel(5)(delayed(cv_run)\n",
    "               (pkldir=pkldir,\n",
    "                extra_savefile_str='percentile',\n",
    "                feature_select_type='percentile',\n",
    "                threshList=perc_List,\n",
    "                covarListList=[''],\n",
    "                cvtype='kfold',\n",
    "                cviter=100)\n",
    "               for pkldir in pkldirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run cross validation permutations\n",
    "for permiter in range(10,21):\n",
    "    Parallel(5)(delayed(cv_run)\n",
    "                (pkldir=pkldir,\n",
    "                 feature_select_type='fpr',\n",
    "                 threshList=pval_fprList,\n",
    "                 covarListList=[''],\n",
    "                 cvtype='kfold',\n",
    "                 cviter=100,\n",
    "                 extra_savefile_str='permtest'+str(permiter),\n",
    "                 permtest=True)\n",
    "                for pkldir in pkldirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmpfiledir=(\n",
    "    datapath+\n",
    "    'cv_results/tmp/'\n",
    "    '_tmp_cv_results_{}_'\n",
    "    'fwhm10_network*fsaverage_df.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save cv results as dataframe\n",
    "cv_fpr_tmpList=(\n",
    "    glob.glob(\n",
    "        tmpfiledir.format('extremepvalsarea')\n",
    "    )+\n",
    "    glob.glob(\n",
    "        tmpfiledir.format('extremepvalsthickness')\n",
    "    )\n",
    ")\n",
    "cv_fpr=[pkl.load(open(tmp,'rb')) \n",
    "        for tmp in cv_fpr_tmpList]\n",
    "save_cv_dfs(\n",
    "    cv_fpr,\n",
    "    'fpr',\n",
    "    endstr='_df_extremepvals_5fold.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save cv results as dataframe\n",
    "cv_fpr_tmpList=(\n",
    "    glob.glob(\n",
    "        tmpfiledir.format('area')\n",
    "    )+\n",
    "    glob.glob(\n",
    "        tmpfiledir.format('thickness')\n",
    "    )\n",
    ")\n",
    "cv_fpr=[pkl.load(open(tmp,'rb')) \n",
    "        for tmp in cv_fpr_tmpList]\n",
    "save_cv_dfs(\n",
    "    cv_fpr,\n",
    "    'fpr',\n",
    "    endstr='_df_5fold.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save NPDATA cv results with covariates as dataframe\n",
    "cv_fpr_npdata_tmpList=glob.glob(\n",
    "    datapath+\n",
    "    'cv_results/tmp/*npdata*.pkl'\n",
    ")\n",
    "cv_fpr_npdata=[pkl.load(open(tmp,'rb')) \n",
    "               for tmp in cv_fpr_npdata_tmpList]\n",
    "save_cv_dfs(\n",
    "    cv_fpr_npdata,\n",
    "    'fpr',\n",
    "    endstr='_df_5fold_npdata.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save cv results with covariates as dataframe\n",
    "cv_fpr_covars_tmpList=glob.glob(\n",
    "    tmpfiledir.format('Gender*')\n",
    ")\n",
    "cv_fpr_covars=[pkl.load(open(tmp,'rb')) \n",
    "               for tmp in cv_fpr_covars_tmpList]\n",
    "save_cv_dfs(\n",
    "    cv_fpr_covars,\n",
    "    'fpr',\n",
    "    endstr='_df_5fold_covars.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save perm cv_results as dataframe\n",
    "for permiter in range(10):\n",
    "    cv_perm_tmpList=glob.glob(\n",
    "        tmpfiledir.format('permtest'+\n",
    "                          str(permiter)+'*')\n",
    "    )\n",
    "    cv_fpr_perm=[pkl.load(open(tmp,'rb')) \n",
    "                 for tmp in cv_perm_tmpList]\n",
    "    save_cv_dfs(\n",
    "        cv_fpr_perm,\n",
    "        'fpr',\n",
    "        endstr=('_df_5fold_permtest{}.csv'\n",
    "                .format(str(permiter)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### make cv features for better interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run for whole brain thickness and area\n",
    "cv_features=[cv_run(pkldir=pkldir,cvtype='kfold',run_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cv_feature_dfs(cv_list):\n",
    "    i_list=[z[0] \n",
    "            for df in cv_list \n",
    "            for x,y,z in df]\n",
    "    i_index=pd.MultiIndex.from_tuples(\n",
    "        i_list,\n",
    "        names=['np_measure_name','pkldir']\n",
    "    )\n",
    "    corrs=[z[1][0] \n",
    "           for df in cv_list \n",
    "           for x,y,z in df]\n",
    "    pvals=[z[1][1] \n",
    "           for df in cv_list \n",
    "           for x,y,z in df]\n",
    "    return (pd.DataFrame(corrs,i_index),\n",
    "            pd.DataFrame(pvals,i_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs_df,pvals_df=make_cv_feature_dfs(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs_df.to_pickle(\n",
    "    datapath+\n",
    "    'cv_results/'\n",
    "    'corrs_df_features_5fold.pkl'\n",
    ")\n",
    "pvals_df.to_pickle(\n",
    "    datapath+\n",
    "    'cv_results/'\n",
    "    'pvals_df_features_5fold.pkl'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
